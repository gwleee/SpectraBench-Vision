--- a/VLMEvalKit/vlmeval/vlm/qwen2_vl/model.py
+++ b/VLMEvalKit/vlmeval/vlm/qwen2_vl/model.py
@@ -222,6 +222,10 @@ class Qwen2VLChatModel(Qwen2VLPromptMixin, BaseModel):
         assert model_path is not None
         self.model_path = model_path
         
+        # Get HuggingFace token from environment
+        token = os.getenv('HUGGING_FACE_HUB_TOKEN') or os.getenv('HF_TOKEN')
+        token_kwargs = {'token': token} if token else {}
+        
         MODEL_CLS = None
 
         if listinstr(['omni'], model_path.lower()):
@@ -231,13 +235,13 @@ class Qwen2VLChatModel(Qwen2VLPromptMixin, BaseModel):
                 logging.critical("pip install git+https://github.com/huggingface/transformers@3a1ead0aabed473eafe527915eea8c197d424356")  # noqa: E501
                 raise err
             MODEL_CLS = Qwen2_5OmniForConditionalGeneration
-            self.processor = Qwen2_5OmniProcessor.from_pretrained(model_path)
+            self.processor = Qwen2_5OmniProcessor.from_pretrained(model_path, **token_kwargs)
         elif listinstr(['2.5', '2_5', 'qwen25', 'mimo'], model_path.lower()):
             from transformers import Qwen2_5_VLForConditionalGeneration, AutoProcessor
             MODEL_CLS = Qwen2_5_VLForConditionalGeneration
-            self.processor = AutoProcessor.from_pretrained(model_path)
+            self.processor = AutoProcessor.from_pretrained(model_path, **token_kwargs)
         else:
             from transformers import Qwen2VLForConditionalGeneration, Qwen2VLProcessor
             MODEL_CLS = Qwen2VLForConditionalGeneration
-            self.processor = Qwen2VLProcessor.from_pretrained(model_path)
+            self.processor = Qwen2VLProcessor.from_pretrained(model_path, **token_kwargs)