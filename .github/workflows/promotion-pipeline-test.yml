name: Promotion Pipeline Tests

on:
  pull_request:
    paths:
      - 'scripts/promote_docker_image.py'
      - 'scripts/automated_promotion.py'
      - 'scripts/monitoring_integration.py'
      - 'scripts/runtime_gpu_smoke_test.py'
      - 'configs/promotion_automation.json'
      - '.github/workflows/docker-promotion-pipeline.yml'
      - '.github/workflows/promotion-pipeline-test.yml'

  push:
    branches: [ main, master ]
    paths:
      - 'scripts/promote_docker_image.py'
      - 'scripts/automated_promotion.py'
      - 'scripts/monitoring_integration.py'
      - 'scripts/runtime_gpu_smoke_test.py'
      - 'configs/promotion_automation.json'

  workflow_dispatch:
    inputs:
      test_scope:
        description: 'Test scope'
        required: true
        default: 'unit'
        type: choice
        options:
          - unit
          - integration
          - full

env:
  PYTHON_VERSION: 3.11

jobs:
  # Job 1: Unit tests for promotion pipeline components
  unit-tests:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        component:
          - promotion_pipeline
          - monitoring_integration
          - automated_promotion
          - smoke_test

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install system dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y docker.io jq

      - name: Install Python dependencies
        run: |
          python -m pip install --upgrade pip
          pip install pytest pytest-mock docker pyyaml psutil nvidia-ml-py3
          # Install additional test dependencies
          pip install responses requests-mock

      - name: Create test configuration
        run: |
          mkdir -p configs outputs
          # Create minimal test config
          cat > configs/test_promotion_automation.json << EOF
          {
            "registry": "ghcr.io/test/spectravision",
            "promotion_schedule": {"enabled": false},
            "ci_cd": {"enabled": true, "trigger_on_push": true},
            "monitoring": {"enabled": false},
            "promotion_rules": {
              "min_age_hours": 1,
              "max_age_days": 1,
              "require_smoke_test": false,
              "require_mini_benchmark": false,
              "dry_run": true
            },
            "versions": ["test-4.33", "test-4.49"]
          }
          EOF

      - name: Run unit tests for promotion pipeline
        if: matrix.component == 'promotion_pipeline'
        run: |
          # Test promotion pipeline without actual Docker operations
          python3 -c "
          import sys
          sys.path.append('.')
          from scripts.promote_docker_image import ImagePromotionPipeline

          # Test initialization
          pipeline = ImagePromotionPipeline('ghcr.io/test/spectravision', enable_monitoring=False)
          assert pipeline.registry == 'ghcr.io/test/spectravision'
          assert pipeline.promotion_history == []
          print('‚úÖ ImagePromotionPipeline initialization test passed')
          "

      - name: Run unit tests for monitoring integration
        if: matrix.component == 'monitoring_integration'
        run: |
          # Test monitoring without actual system monitoring
          python3 -c "
          import sys
          sys.path.append('.')
          try:
            from scripts.monitoring_integration import MonitoringIntegration, SystemMetrics

            # Test SystemMetrics structure
            metrics = SystemMetrics()
            assert hasattr(metrics, 'cpu')
            assert hasattr(metrics, 'memory')
            assert hasattr(metrics, 'disk')
            print('‚úÖ MonitoringIntegration structure test passed')
          except ImportError as e:
            print('‚ö†Ô∏è Monitoring dependencies not available, skipping test')
          "

      - name: Run unit tests for automated promotion
        if: matrix.component == 'automated_promotion'
        run: |
          # Test automated promotion configuration
          python3 -c "
          import sys
          sys.path.append('.')
          from scripts.automated_promotion import AutomatedPromotion

          # Test with test config
          automation = AutomatedPromotion('configs/test_promotion_automation.json')
          assert automation.config['registry'] == 'ghcr.io/test/spectravision'
          assert automation.config['versions'] == ['test-4.33', 'test-4.49']
          print('‚úÖ AutomatedPromotion configuration test passed')

          # Test candidate filtering logic
          candidates = [
            {'tag': 'test:4.33-candidate', 'version': '4.33', 'age_hours': 2},
            {'tag': 'test:4.49-candidate', 'version': '4.49', 'age_hours': 0.5}
          ]

          filtered = automation.filter_candidates_by_rules(candidates)
          assert len(filtered) == 1  # Only one should pass age filter (min_age_hours: 1)
          assert filtered[0]['version'] == '4.33'
          print('‚úÖ Candidate filtering test passed')
          "

      - name: Run unit tests for smoke test
        if: matrix.component == 'smoke_test'
        run: |
          # Test smoke test structure
          python3 -c "
          import sys
          sys.path.append('.')
          from scripts.runtime_gpu_smoke_test import run_gpu_smoke_test

          # Test function exists and returns expected structure
          try:
            success, results = run_gpu_smoke_test()
            assert isinstance(success, bool)
            assert isinstance(results, dict)
            assert 'tests' in results
            assert 'summary' in results
            print('‚úÖ GPU smoke test structure passed')
          except Exception as e:
            print(f'‚ö†Ô∏è GPU smoke test failed (expected in CI without GPU): {e}')
          "

  # Job 2: Integration tests
  integration-tests:
    if: |
      github.event.inputs.test_scope == 'integration' ||
      github.event.inputs.test_scope == 'full'
    runs-on: ubuntu-latest

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y docker.io jq
          python -m pip install --upgrade pip
          pip install docker pyyaml psutil nvidia-ml-py3

      - name: Log in to Container Registry
        uses: docker/login-action@v3
        with:
          registry: ghcr.io
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}

      - name: Test candidate discovery
        run: |
          # Test automated promotion candidate scanning
          python3 scripts/automated_promotion.py --mode list --json > test_candidates.json

          # Verify JSON structure
          if jq -e '.total_candidates' test_candidates.json > /dev/null; then
            echo "‚úÖ Candidate discovery test passed"
            TOTAL=$(jq -r '.total_candidates' test_candidates.json)
            ELIGIBLE=$(jq -r '.eligible_candidates' test_candidates.json)
            echo "Found $TOTAL total candidates, $ELIGIBLE eligible"
          else
            echo "‚ùå Candidate discovery test failed"
            cat test_candidates.json
            exit 1
          fi

      - name: Test promotion pipeline dry-run
        run: |
          # Create a mock candidate tag for testing
          MOCK_TAG="ghcr.io/gwleee/spectravision:test-candidate"

          # Test promotion pipeline with skip-tests (no GPU required)
          if python3 scripts/promote_docker_image.py \
            "$MOCK_TAG" \
            --skip-tests \
            --json > test_promotion_result.json 2>/dev/null; then

            echo "‚úÖ Promotion pipeline structure test passed"
            cat test_promotion_result.json | jq .
          else
            echo "‚ö†Ô∏è Promotion pipeline test failed (expected - no actual candidate image)"
            echo "This is expected behavior in CI environment"
          fi

      - name: Test monitoring system
        run: |
          # Test monitoring integration without GPU
          python3 -c "
          import sys
          sys.path.append('.')
          try:
            from scripts.monitoring_integration import MonitoringIntegration
            monitor = MonitoringIntegration('/tmp/test_monitoring')
            # Test basic monitoring setup
            print('‚úÖ Monitoring integration test passed')
          except Exception as e:
            print(f'‚ö†Ô∏è Monitoring test skipped: {e}')
          "

  # Job 3: Full system tests (Docker integration)
  system-tests:
    if: |
      github.event.inputs.test_scope == 'full'
    runs-on: ubuntu-latest

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y docker.io jq
          python -m pip install --upgrade pip
          pip install docker pyyaml psutil nvidia-ml-py3

      - name: Log in to Container Registry
        uses: docker/login-action@v3
        with:
          registry: ghcr.io
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}

      - name: Test Docker connectivity
        run: |
          # Test that we can communicate with Docker daemon
          docker version
          docker info
          echo "‚úÖ Docker connectivity test passed"

      - name: Test container inspection
        run: |
          # Test container manifest inspection (used by promotion pipeline)
          for VERSION in "4.33" "4.37" "4.43" "4.49" "4.51"; do
            TAG="ghcr.io/gwleee/spectravision:$VERSION"
            echo "Testing manifest inspection for $TAG..."

            if docker manifest inspect "$TAG" > /dev/null 2>&1; then
              echo "‚úÖ Can inspect $TAG"
            else
              echo "‚ö†Ô∏è Cannot inspect $TAG (may not exist or require authentication)"
            fi
          done

      - name: Test promotion system end-to-end
        run: |
          # Run comprehensive system test
          echo "üîß Running end-to-end promotion system test..."

          # Test with dry-run configuration
          python3 scripts/automated_promotion.py \
            --mode batch \
            --json > system_test_results.json

          # Verify results structure
          if jq -e '.timestamp' system_test_results.json > /dev/null; then
            echo "‚úÖ End-to-end system test passed"

            SUCCESSFUL=$(jq -r '.successful_promotions // 0' system_test_results.json)
            FAILED=$(jq -r '.failed_promotions // 0' system_test_results.json)
            TOTAL=$(jq -r '.eligible_candidates // 0' system_test_results.json)

            echo "System test results: $SUCCESSFUL successful, $FAILED failed, $TOTAL total candidates"
          else
            echo "‚ùå End-to-end system test failed"
            cat system_test_results.json
            exit 1
          fi

      - name: Upload test results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: system-test-results-${{ github.run_number }}
          path: |
            test_*.json
            system_test_results.json
          retention-days: 7

  # Job 4: Performance and security tests
  quality-tests:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install analysis tools
        run: |
          python -m pip install --upgrade pip
          pip install bandit safety pylint mypy
          pip install docker pyyaml psutil nvidia-ml-py3

      - name: Run security analysis
        run: |
          echo "üîí Running security analysis..."

          # Check for security vulnerabilities in promotion scripts
          bandit -r scripts/promote_docker_image.py scripts/automated_promotion.py scripts/monitoring_integration.py -f json -o bandit_results.json || true

          if [ -f bandit_results.json ]; then
            ISSUES=$(jq '.metrics._totals.CONFIDENCE.HIGH // 0' bandit_results.json)
            if [ "$ISSUES" -gt 0 ]; then
              echo "‚ö†Ô∏è High-confidence security issues found: $ISSUES"
              jq '.results[]?' bandit_results.json
            else
              echo "‚úÖ No high-confidence security issues found"
            fi
          fi

      - name: Run dependency safety check
        run: |
          echo "üõ°Ô∏è Checking dependencies for known vulnerabilities..."

          # Create requirements list from imports
          echo "docker" > temp_requirements.txt
          echo "pyyaml" >> temp_requirements.txt
          echo "psutil" >> temp_requirements.txt

          safety check -r temp_requirements.txt || true
          echo "‚úÖ Dependency safety check completed"

      - name: Run static analysis
        run: |
          echo "üîç Running static analysis..."

          # Lint promotion scripts
          pylint scripts/promote_docker_image.py || true
          pylint scripts/automated_promotion.py || true
          pylint scripts/monitoring_integration.py || true

          echo "‚úÖ Static analysis completed"

      - name: Check configuration security
        run: |
          echo "‚öôÔ∏è Checking configuration security..."

          # Verify no secrets in configs
          if grep -r "password\|secret\|token\|key" configs/ 2>/dev/null | grep -v "null\|false\|\[\]"; then
            echo "‚ùå Potential secrets found in configuration files"
            exit 1
          else
            echo "‚úÖ No secrets detected in configuration files"
          fi

          # Check for secure defaults
          if jq -e '.promotion_rules.dry_run == false' configs/promotion_automation.json > /dev/null; then
            echo "‚ö†Ô∏è Warning: dry_run is disabled in production config"
          else
            echo "‚úÖ Safe dry_run configuration detected"
          fi

      - name: Upload quality results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: quality-test-results-${{ github.run_number }}
          path: |
            bandit_results.json
            temp_requirements.txt
          retention-days: 7