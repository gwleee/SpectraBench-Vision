# SpectraVision Environment Variables Template
# Copy this file to .env and fill in your personal API keys and configurations

# =============================================================================
# GITHUB CONTAINER REGISTRY
# =============================================================================
# GitHub Personal Access Token for Container Registry access
# Get your token from: https://github.com/settings/tokens
GITHUB_TOKEN=your_github_token_here

# =============================================================================
# HUGGING FACE CONFIGURATION
# =============================================================================
# Required for accessing gated models like MiniCPM-V-2.6
# Get your token from: https://huggingface.co/settings/tokens
# Create a token with "Read" permissions
HF_TOKEN=your_huggingface_token_here

# =============================================================================
# OPENAI API (Optional)
# =============================================================================
# If using OpenAI GPT models for evaluation
# Get your API key from: https://platform.openai.com/api-keys
OPENAI_API_KEY=your_openai_api_key_here

# =============================================================================
# CUDA CONFIGURATION
# =============================================================================
# Specify which GPU(s) to use (comma-separated for multiple GPUs)
# Examples:
#   Single GPU:    CUDA_VISIBLE_DEVICES=0
#   Dual GPU:      CUDA_VISIBLE_DEVICES=0,1
#   Quad GPU:      CUDA_VISIBLE_DEVICES=0,1,2,3
#   Octo GPU:      CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
#   All GPUs:      CUDA_VISIBLE_DEVICES=all
#   Specific GPUs: CUDA_VISIBLE_DEVICES=0,2,4,6,8,10,12,14
CUDA_VISIBLE_DEVICES=0

# Number of GPUs to use for Docker containers (optional)
# System will auto-detect if not specified
# Supports unlimited GPU count (tested up to 64+ GPUs)
GPU_COUNT=1

# =============================================================================
# EVALUATION SETTINGS
# =============================================================================
# Default batch size for model evaluation
# For multi-GPU setups, you may increase this value
VLMEVAL_BATCH_SIZE=1

# Enable multi-GPU model distribution for large models (true/false)
ENABLE_MULTI_GPU_DISTRIBUTION=false

# Minimum GPU count required for large model evaluation
MIN_GPU_COUNT_FOR_LARGE_MODELS=2

# Enable cluster mode for 10+ GPU setups (true/false)
ENABLE_CLUSTER_MODE=false

# Maximum GPU count to use (0 = unlimited)
MAX_GPU_COUNT=0

# =============================================================================
# DOCKER COMPOSE SETTINGS
# =============================================================================
# Shared memory sizes for different transformer versions (adjustable per hardware)
SHM_SIZE_LEGACY=8gb          # For transformers 4.33.0 (legacy models)
SHM_SIZE_STABLE=16gb         # For transformers 4.37.2 (stable models) 
SHM_SIZE_MODERN=8gb          # For transformers 4.43.0 (modern models)
SHM_SIZE_LATEST=16gb         # For transformers 4.49.0 (latest models)
SHM_SIZE_CUTTING_EDGE=32gb   # For transformers 4.51.0 (cutting-edge models)

# Enable/disable HuggingFace datasets offline mode (0=online, 1=offline)
HF_DATASETS_OFFLINE=0

# Enable/disable transformers offline mode (0=online, 1=offline)
TRANSFORMERS_OFFLINE=0

# =============================================================================
# LOGGING AND MONITORING
# =============================================================================
# Log level (DEBUG, INFO, WARNING, ERROR)
LOG_LEVEL=INFO

# Enable performance monitoring (true/false)
ENABLE_MONITORING=false

# =============================================================================
# CACHE CONFIGURATION
# =============================================================================
# Cache cleanup level (light, moderate, aggressive)
CACHE_CLEANUP_LEVEL=light

# Enable automatic cache cleanup after each evaluation (true/false)
ENABLE_CACHE_CLEANUP=true

# =============================================================================
# OUTPUT CONFIGURATION
# =============================================================================
# Default output directory for results and logs
OUTPUT_DIR=outputs

# Maximum log file size in MB
MAX_LOG_SIZE=100

# Number of log files to keep
LOG_FILE_COUNT=5